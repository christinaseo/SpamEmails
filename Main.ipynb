{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple Classification of Spam Emails"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ham files\n",
    "##Import data to build the dictionary\n",
    "filtered = []\n",
    "stopwords = set(stopwords.words('english'))\n",
    "for filename in os.listdir('enron1/ham'):\n",
    "    # read each file, if it is a new word add it to the dictionary and keep a count of the lengths\n",
    "    file = open('enron1/ham/'+filename,encoding='utf-8',errors = 'ignore').read()\n",
    "    tokens = nltk.word_tokenize(file)\n",
    "    for w in tokens:\n",
    "        if w not in stopwords:\n",
    "            if w.isalpha():\n",
    "                filtered.append(w.lower())\n",
    "freq = nltk.FreqDist(filtered)\n",
    "hdict = []\n",
    "for i in freq.most_common(1000):\n",
    "    hdict.append(i[0])\n",
    "ham_probability={}\n",
    "tempTokens=[]\n",
    "for filename in os.listdir('enron1/ham'):\n",
    "    file = open('enron1/ham/'+filename,encoding='utf-8',errors = 'ignore').read()\n",
    "    tokens = nltk.word_tokenize(file.lower())\n",
    "    lTokens = set(tokens)\n",
    "    for w in lTokens:\n",
    "        if w not in stopwords:\n",
    "            if w in hdict:\n",
    "                if not w in ham_probability:\n",
    "                    ham_probability[w]=1\n",
    "                else:\n",
    "                    ham_probability[w]+=1      \n",
    "                    \n",
    "#print(ham_probability['subject'])\n",
    " \n",
    "for key in ham_probability.keys():           \n",
    "    ham_probability[key] = ham_probability[key] / 3672      \n",
    "\n",
    "#print(ham_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spam files\n",
    "##Import data to build the dictionary\n",
    "sfiltered = []\n",
    "stopwords = set(stopwords.words('english'))\n",
    "for filename in os.listdir('enron1/spam'):\n",
    "    # read each file, if it is a new word add it to the dictionary and keep a count of the lengths\n",
    "    file = open('enron1/spam/'+filename,encoding='utf-8',errors = 'ignore').read()\n",
    "    tokens = nltk.word_tokenize(file)\n",
    "    for w in tokens:\n",
    "        if w not in stopwords:\n",
    "            if w.isalpha():\n",
    "                sfiltered.append(w.lower())\n",
    "freq = nltk.FreqDist(sfiltered)\n",
    "sdict = []\n",
    "for i in freq.most_common(1000):\n",
    "    sdict.append(i[0])\n",
    "spam_probability={}\n",
    "stempTokens=[]\n",
    "for filename in os.listdir('enron1/spam'):\n",
    "    file = open('enron1/spam/'+filename,encoding='utf-8',errors = 'ignore').read()\n",
    "    stokens = nltk.word_tokenize(file.lower())\n",
    "    slTokens = set(stokens)\n",
    "    for w in slTokens:\n",
    "        if w not in stopwords:\n",
    "            if w in sdict:\n",
    "                if not w in spam_probability:\n",
    "                    spam_probability[w]=1\n",
    "                else:\n",
    "                    spam_probability[w]+=1      \n",
    "                    \n",
    "#print(spam_probability['subject'])\n",
    " \n",
    "for key in spam_probability.keys():           \n",
    "    spam_probability[key] = spam_probability[key] / 1500      \n",
    "\n",
    "#print(spam_probability)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the new file and tokenize\n",
    "stopwords = set(stopwords.words('english'))\n",
    "file = open('test.txt',encoding='utf-8',errors = 'ignore').read()\n",
    "inputTokens = nltk.word_tokenize(file)\n",
    "\n",
    "#Loop through the dictionary and calculate the probability\n",
    "#calculate the spam portion\n",
    "spamProb = 1\n",
    "for s in sdict:\n",
    "    if s in inputTokens:\n",
    "        spamProb = spamProb * spam_probability[s]\n",
    "    else:\n",
    "        spamProb = spamProb * (1 - spam_probability[s])\n",
    "spamProb = spamProb * (1/4)\n",
    "\n",
    "#calculate the ham portion\n",
    "hamProb = 1\n",
    "for h in hdict:\n",
    "    if h in inputTokens:\n",
    "        hamProb = hamProb * ham_probability[s]\n",
    "    else:\n",
    "        hamProb = hamProb * (1 - ham_probability[s])\n",
    "hamProb = hamProb * (3/4)\n",
    "\n",
    "#Calculate inference\n",
    "inference = spamProb/(spamProb + hamProb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "3.0716965904182334e-34\n"
     ]
    }
   ],
   "source": [
    "print(inference)\n",
    "print(spamProb)\n",
    "print(hamProb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
